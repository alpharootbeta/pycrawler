
目标：让用户用最简单的方式获得目标网站的数据，不必为每个网站单独编写爬虫。

特性：
1、支持多线程爬取目标网站
2、只需要在configure.py中配置好目标网站的信息即可爬取

待增加特性：
1、为了防止被目标网站墙掉，需要增加代理服务器功能
2、某些信息需要登陆后才能抓取，需要增加自动登陆模块
3、增加单独的日志处理和抽取信息的处理模块，目前是直接打印出来

使用方法：

配置好之后直接实例化并执行Crawler类的crawl()方法即可。


依赖的第三方库:

BeautifulSoup
lxml
